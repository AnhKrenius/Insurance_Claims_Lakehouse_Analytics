# Insurance Claims Analytics Platform
**End-to-End Lakehouse Analytics Project (Azure Databricks Â· Delta Lake Â· Power BI)**

This project demonstrates a **production-style data engineering and analytics workflow**, covering data ingestion, validation, transformation, optimization, and business reporting using a **Medallion (Bronze / Silver / Gold) Lakehouse architecture**.

The focus of the project is not only dashboarding, but **building a reliable, performant, and analytics-ready data foundation** that supports downstream BI use cases.

---

## ğŸ¯ Project Objectives

- Design a scalable **Lakehouse architecture** suitable for real-world insurance analytics
- Implement robust **data quality checks, enrichment, and traceability**
- Build **fact and dimension models** optimized for BI consumption
- Apply **Delta Lake performance optimizations** and measure their impact
- Deliver management-focused **Power BI dashboards** on top of curated Gold tables

---

## ğŸ—ï¸ Architecture Overview

**Core Technologies**
- **Azure Data Lake Gen2** â€“ cloud storage for Bronze, Silver, and Gold layers
- **Azure Databricks (Spark)** â€“ distributed processing and Delta Lake management
- **Delta Lake** â€“ ACID transactions, schema enforcement, versioning
- **Power BI** â€“ analytics and executive reporting
- **Git** â€“ version control and reproducibility

**Architecture Pattern**
```
Raw Files â†’ Bronze (Raw Delta)
â†’ Silver (Cleaned, Validated, Enriched)
â†’ Gold (Fact & Dimension Tables, Optimized)
â†’ Power BI Dashboards
```

---

## ğŸ”„ Data Pipeline Breakdown

### ğŸŸ« Bronze Layer â€” Raw Ingestion
- Ingested raw CSV datasets (Claims, Customers, Policies, Handlers)
- Stored as **Delta tables** in ADLS to enable:
  - Schema evolution
  - Time travel
  - Auditability
- No business logic applied at this stage (raw-by-design)

---

### ğŸŸª Silver Layer â€” Data Engineering Core (Key Strength)

The **Silver layer is the heart of this project** and where most data engineering work happens.

Implemented transformations include:

#### âœ… Data Quality & Validation
- Dropped records with missing primary keys
- Removed duplicate records based on business keys
- Validated date logic (e.g. create date â‰¤ close date)
- Detected invalid numerical values (negative claim amounts, outliers)

#### ğŸš© Data Quality Flags (Not Hard Deletes)
Instead of blindly dropping data, **data quality issues are explicitly flagged**, enabling:
- Transparency
- Auditability
- Downstream decision-making

Example flags:
- `IsInvalidClaimDate`
- `IsNegativeClaimAmount`
- `IsOutlierClaimAmount`
- `IsMissingCustomer`
- `IsMissingPolicy`

This mirrors **real enterprise data engineering practices**.

#### ğŸ§  Enrichment & Feature Engineering
- Derived metrics:
  - Claim duration (days)
  - Customer age
  - Policy tenure
- Standardized categorical fields
- Prepared analytical features for BI and future ML use cases

---

### ğŸŸ¨ Gold Layer â€” Analytics-Ready Modeling

The Gold layer exposes **business-ready datasets** optimized for analytics.

#### â­ Star Schema Design
- **FactClaim**
- **DimCustomer**
- **DimPolicy**
- **DimHandler**
- **DimDate**

Benefits:
- Simplified BI modeling
- Faster query performance
- Clear separation of facts and dimensions

#### ğŸ§¾ Table Registration
Gold Delta tables are **registered in Databricks metastore**, enabling:
- SQL access
- BI tool connectivity
- Centralized schema governance

---

## âš¡ Performance Optimization & Benchmarking

Gold tables were optimized using **Delta Lake best practices**:

### Optimization Techniques
- `OPTIMIZE` for file compaction
- `ZORDER BY (CustomerID, PolicyNumber)` for data skipping
- Metadata cleanup via `VACUUM`

### Measured Results (FactClaim Table)
| Query Type | Performance Improvement |
|----------|--------------------------|
| Read Queries | **61% faster** |
| Filter Queries | **74% faster** |
| Aggregations | **64% faster** |

Performance was **measured before and after optimization** using Databricks benchmarking notebooks.

These improvements translated into:
- Faster Power BI dataset refresh
- More responsive interactive filtering
- Reduced query latency

---

## ğŸ“Š Power BI Dashboards

Power BI dashboards were built directly on curated Gold tables and focus on **operational and management insights**, not raw reporting.

### Key Analytics Areas
- **Claims SLA compliance**
  - Assignment within 3 days
  - Closure within 30 days
- **Claim resolution duration**
- **Incident type & multi-vehicle claim analysis**
- **Handler and reporting manager efficiency**
- **Customer segmentation (Platinum focus)**
- **Geographic distribution of customers and claims**

### Dashboard Screenshots

#### Claims Overview
![Claims Overview](powerbi/screenshots/claim_overview.png)

#### Manager & Efficiency
![Manager Efficiency](powerbi/screenshots/manager_efficiency.png)

---

## ğŸ“ Repository Structure

```
insurance-claims-analytics/
â”œâ”€â”€ README.md
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ 01_bronze_ingestion.ipynb
â”‚ â”œâ”€â”€ 02_silver_transformation.ipynb
â”‚ â”œâ”€â”€ 03_gold_modeling.ipynb
â”‚ â”œâ”€â”€ 04_register_gold_tables.ipynb
â”‚ â””â”€â”€ 05_optimize_and_benchmark_factclaim.ipynb
â”œâ”€â”€ powerbi/
â”‚ â”œâ”€â”€ screenshots/
â”‚ â””â”€â”€ dashboard_description.md
â”œâ”€â”€ docs/
â”‚ â”œâ”€â”€ data_model.md
â”‚ â”œâ”€â”€ kpi_definitions.md
â”‚ â””â”€â”€ performance_benchmark.md
â””â”€â”€ architecture/
```


---

## ğŸ” Reproducibility & Engineering Practices

- Modular notebooks with clear execution order
- Explicit separation of Bronze / Silver / Gold logic
- Version-controlled code and documentation
- Metrics-driven performance validation
- Architecture and KPI documentation included

---

## ğŸ“Œ Notes

Cloud resources used for development are no longer active.  
This repository preserves the **full engineering design, transformation logic, performance benchmarks, and BI outputs** for portfolio and demonstration purposes.

---

## ğŸ‘¤ Target Roles

This project is suitable for:
- Data Engineer
- Analytics Engineer
- BI Engineer
- Senior Data Analyst (with engineering focus)

